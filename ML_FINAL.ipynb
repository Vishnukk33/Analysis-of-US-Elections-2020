{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of DataFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Creating ML.csv File to be used below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "final_file=pd.read_csv('ML.csv')\n",
    "i=1\n",
    "for name in range(1,6):\n",
    "    print(name)\n",
    "    data=pd.read_csv('democ'+str(name)+'.csv')\n",
    "    rows=len(data)\n",
    "    index_set=set()\n",
    "    while len(index_set)<3000:\n",
    "        index=random.randint(0, rows-1)\n",
    "        if index not in index_set:\n",
    "            index_set.add(index)\n",
    "            final_file['Tweet'][i]=data['tweetText'][index]\n",
    "            final_file['Party'][i]=0\n",
    "            i+=1\n",
    "        else:\n",
    "            continue\n",
    "for name in range(1,6):\n",
    "    print(name)\n",
    "    data=pd.read_csv('repub'+str(name)+'.csv')\n",
    "    rows=len(data)\n",
    "    index_set=set()\n",
    "    while len(index_set)<3000:\n",
    "        index=random.randint(0, rows-1)\n",
    "        if index not in index_set:\n",
    "            index_set.add(index)\n",
    "            final_file['Tweet'][i]=data['tweetText'][index]\n",
    "            final_file['Party'][i]=1\n",
    "            i+=1\n",
    "        else:\n",
    "            continue\n",
    "final_file.to_csv('ML.csv',index='False')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vishn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Performing Preprocessing\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "nltk.download('stopwords')\n",
    "csvname='ML.csv'\n",
    "data=pd.read_csv(csvname)\n",
    "data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "rows=len(data)\n",
    "\n",
    "# loop to clean the tweets and to remove stopwords and perform stemming.\n",
    "for i in range (0,rows):\n",
    "    oldtext=data['Tweet'][i] \n",
    "    newtext=' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(oldtext)).split()) # removes hashtags, https links, usernames and any other alpha-numeric characters.\n",
    "    newtext= re.sub('[^a-zA-Z]',\" \",str(newtext)) #removes all other characters other than alphabets\n",
    "    newtext=newtext.lower()\n",
    "    newtext=newtext.split()\n",
    "    ps=PorterStemmer()\n",
    "    newtext= [ps.stem(word) for word in newtext if word not in set(stopwords.words('english'))]\n",
    "    newtext=' '.join(newtext)\n",
    "    #print(newtext)\n",
    "    data['Tweet'][i]=newtext\n",
    "        \n",
    "# Saving changes to the csv file\n",
    "data.to_csv(csvname,index='False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from vecstack import stacking\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "csvname=\"ML.csv\" # Insert your csv name here.\n",
    "data=pd.read_csv(csvname)\n",
    "rows=len(data)\n",
    "\n",
    "# 'corpus' list consists of the tweet text and 'y' consists of the political affliation of the tweet.\n",
    "\n",
    "corpus=[]\n",
    "y=[]\n",
    "for i in range(rows):\n",
    "    newtext=data['Tweet'][i]\n",
    "    if(len(str((newtext)))!=0 and str(newtext)!='nan'):\n",
    "        corpus.append(newtext)\n",
    "        y.append(data['Party'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Term-Frequency-Iverse-Document-Frequency and fitting corpus into it.\n",
    "# Data-set 1 (X1) created using TF-IDF\n",
    "vectorizer = TfidfVectorizer(min_df=50,max_features=1500)\n",
    "vectorizer.fit(corpus)\n",
    "X1 = vectorizer.transform(corpus)\n",
    "X1=X1.toarray()\n",
    "\n",
    "# Using Count-Vectorizer and fitting corpus into it.\n",
    "# Data-set 2 (X2) created using Count-Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X2 = cv.fit_transform(corpus).toarray()\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into Training and Test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size = 0.2, random_state = 0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing LinearDiscriminantAnalysis.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components =1)\n",
    "X1_train = lda.fit_transform(X1_train, y1_train)\n",
    "X1_test = lda.transform(X1_test)\n",
    "lda = LDA(n_components =1)\n",
    "X2_train = lda.fit_transform(X2_train, y2_train)\n",
    "X2_test = lda.transform(X2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the data into Machine Learning Models.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "SVC1 = SVC(kernel = 'linear', random_state = 0)\n",
    "SVC1.fit(X1_train, y1_train)\n",
    "\n",
    "SVC2 = SVC(kernel = 'linear', random_state = 0)\n",
    "SVC2.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "                   base_estimator=SVC(kernel='linear', random_state=0),\n",
       "                   learning_rate=1, random_state=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting the data into AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboostSVC1 = AdaBoostClassifier(n_estimators=50, base_estimator= SVC1,learning_rate=1, random_state = 1,algorithm='SAMME')\n",
    "adaboostSVC1.fit(X1_train,y1_train)\n",
    "\n",
    "adaboostSVC2 = AdaBoostClassifier(n_estimators=50, base_estimator= SVC2,learning_rate=1, random_state = 1,algorithm='SAMME')\n",
    "adaboostSVC2.fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the data into XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "XGB1 = XGBClassifier()\n",
    "XGB1.fit(X1_train, y1_train)\n",
    "\n",
    "XGB2 = XGBClassifier()\n",
    "XGB2.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the data into Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF1 = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "RF1.fit(X1_train, y1_train)\n",
    "RF2 = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "RF2.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the data into Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT1 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DT1.fit(X1_train, y1_train)\n",
    "\n",
    "DT2= DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DT2.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy using Decision Tree on Data-set 1:  0.6096498575975875\n",
      "The Accuracy using Decision Tree on Data-set 2:  0.6191991958452002\n",
      "The Accuracy using Extreme Gradient Boost on Data-set 1:  0.6463394203384152\n",
      "The Accuracy using Extreme Gradient Boost on Data-set 2:  0.6557212263360697\n",
      "The Accuracy using Random Forest on Data-set 1:  0.6119953090970012\n",
      "The Accuracy using Random Forest on Data-set 2:  0.6193667280951584\n",
      "The Accuracy using Support Vector Clustering on Data-set 1:  0.6513653878371587\n",
      "The Accuracy using Support Vector Clustering on Data-set 2:  0.6580666778354833\n",
      "The Accuracy using AdaBoosted Support Vector Clustering on Data-set 1:  0.6481822750879545\n",
      "The Accuracy using AdaBoosted Support Vector Clustering on Data-set 2:  0.6560562908359859\n"
     ]
    }
   ],
   "source": [
    "#Predicting Outputs of the different classifiers\n",
    "\n",
    "y_pred=DT1.predict(X1_test)\n",
    "print(\"The Accuracy using Decision Tree on Data-set 1: \",accuracy_score(y1_test,y_pred))\n",
    "\n",
    "y_pred=DT2.predict(X2_test)\n",
    "print(\"The Accuracy using Decision Tree on Data-set 2: \",accuracy_score(y2_test,y_pred)) \n",
    "\n",
    "y_pred=XGB1.predict(X1_test)\n",
    "print(\"The Accuracy using Extreme Gradient Boost on Data-set 1: \",accuracy_score(y1_test,y_pred))\n",
    "\n",
    "y_pred=XGB2.predict(X2_test)\n",
    "print(\"The Accuracy using Extreme Gradient Boost on Data-set 2: \",accuracy_score(y2_test,y_pred)) \n",
    "\n",
    "y_pred=RF1.predict(X1_test)\n",
    "print(\"The Accuracy using Random Forest on Data-set 1: \",accuracy_score(y1_test,y_pred)) \n",
    "\n",
    "y_pred=RF2.predict(X2_test)\n",
    "print(\"The Accuracy using Random Forest on Data-set 2: \",accuracy_score(y2_test,y_pred))\n",
    "\n",
    "y_pred=SVC1.predict(X1_test)\n",
    "print(\"The Accuracy using Support Vector Clustering on Data-set 1: \",accuracy_score(y1_test,y_pred))\n",
    "\n",
    "y_pred=SVC2.predict(X2_test)\n",
    "print(\"The Accuracy using Support Vector Clustering on Data-set 2: \",accuracy_score(y2_test,y_pred))\n",
    "\n",
    "y_pred=adaboostSVC1.predict(X1_test)\n",
    "print(\"The Accuracy using AdaBoosted Support Vector Clustering on Data-set 1: \",accuracy_score(y1_test,y_pred))\n",
    "\n",
    "y_pred=adaboostSVC2.predict(X2_test)\n",
    "print(\"The Accuracy using AdaBoosted Support Vector Clustering on Data-set 2: \",accuracy_score(y2_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy using Voting Classifier on Data-set 1:  0.6414809850896298\n",
      "The Accuracy using Voting Classifier on Data-set 2:  0.6525381135868654\n"
     ]
    }
   ],
   "source": [
    "# Performing Voting to improve accuracy. \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "     ('RF1',RF1),('adaboostSVC1',adaboostSVC1),('XGB1',XGB1)], voting='soft')\n",
    "eclf1.fit(X1_train, y1_train)\n",
    "y1_pred=eclf1.predict(X1_test)\n",
    "print(\"The Accuracy using Voting Classifier on Data-set 1: \",accuracy_score(y1_test,y1_pred)) \n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[\n",
    "     ('RF2',RF2),('adaboostSVC2',adaboostSVC2),('XGB2',XGB2)], voting='soft')\n",
    "eclf2.fit(X2_train, y2_train)\n",
    "y2_pred=eclf2.predict(X2_test)\n",
    "print(\"The Accuracy using Voting Classifier on Data-set 2: \",accuracy_score(y2_test,y2_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
